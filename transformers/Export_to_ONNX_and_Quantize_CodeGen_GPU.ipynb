{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vrBEWXkZGAis"
      },
      "source": [
        "# CodeGen Model Optimization on GPU\n",
        "The code in this notebook is to do optimization and quantization of the SalesForce's [Codegen](https://github.com/salesforce/CodeGen) mono model in a Colab VM with hardware acceleration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGTtyWYQGOF7"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPro_VydGP2k"
      },
      "source": [
        "Check for the GPU model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ZWJD5wyQfz"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0a9RhHMGTsD"
      },
      "source": [
        "Install the missing requirements in the Colab VM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feOSSQy500_t"
      },
      "outputs": [],
      "source": [
        "!pip install transformers onnx onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek_1fkd1GX2C"
      },
      "source": [
        "Download the CodeGen pre-trained model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHRjINkqpi5L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "  device = \"cuda\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer_id = \"Salesforce/codegen-350M-mono\"\n",
        "model_id = \"Salesforce/codegen-350M-mono\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYrw3gemqzr_"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"local-pt-checkpoint\")\n",
        "model.save_pretrained(\"local-pt-checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLEhPBA5GkgO"
      },
      "source": [
        "### Conversion to ONNX Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHiA3udRGu2B"
      },
      "source": [
        "Convert the pre-trained model to the ONNX format using the tool available in the Transformers library. The command below performs also validation at the end of the conversion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6REXR-NZq22e"
      },
      "outputs": [],
      "source": [
        "!python -m transformers.onnx --feature \"causal-lm\" --framework pt --export_with_transformers --model=local-pt-checkpoint onnx/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Qz1eI-5zpe"
      },
      "source": [
        "### Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtqLH-8DtzW9"
      },
      "outputs": [],
      "source": [
        "onnx_model_path = \"onnx/model.onnx\"\n",
        "quantized_model_path = \"model.quant.onnx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Qi0Gaciqyw"
      },
      "source": [
        "Define some utility functions to perform benchmarks of different versions of the model with diverse providers in the ONNX runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0TIpRSc2FNl"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from time import time\n",
        "from tqdm import trange\n",
        "from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_all_providers\n",
        "\n",
        "def create_model_for_provider(model_path, provider, enable_profiling=False):\n",
        "\n",
        "  assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
        "\n",
        "  # Few properties that might have an impact on performances (provided by MS)\n",
        "  options = SessionOptions()\n",
        "  options.enable_profiling = enable_profiling\n",
        "  options.intra_op_num_threads = 1\n",
        "  options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "\n",
        "  # Load the model as a graph and prepare the CPU backend\n",
        "  session = InferenceSession(model_path, options, providers=[provider])\n",
        "  session.disable_fallback()\n",
        "\n",
        "  return session\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def track_infer_time(buffer: [int]):\n",
        "    start = time()\n",
        "    yield\n",
        "    end = time()\n",
        "\n",
        "    buffer.append(end - start)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OnnxInferenceResult:\n",
        "  model_inference_time: [int]\n",
        "  optimized_model_path: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk52zg-_ielQ"
      },
      "source": [
        "Prepare the input to use for benchmarking the original model (PyTorch Tensor) and the ONNX versions (numpy array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlufWXIr0vky"
      },
      "outputs": [],
      "source": [
        "from transformers import CodeGenTokenizerFast\n",
        "\n",
        "tokenizer = CodeGenTokenizerFast.from_pretrained(model_id)\n",
        "\n",
        "prompt = \"def create_bar_chart_with_matplotlib():\"\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rll78-9_5-RQ"
      },
      "source": [
        "Benchmark PyTorch on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8O2tJvt51am"
      },
      "outputs": [],
      "source": [
        "from transformers import CodeGenModel\n",
        "\n",
        "PROVIDERS = {\n",
        "    (\"cuda:0\", \"PyTorch GPU\")\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for device, label in PROVIDERS:\n",
        "\n",
        "    model_inputs_on_device = {\n",
        "        arg_name: tensor.to(device)\n",
        "        for arg_name, tensor in model_inputs.items()\n",
        "    }\n",
        "\n",
        "    model_pt = CodeGenModel.from_pretrained(model_id).to(device)\n",
        "    for _ in trange(10, desc=\"Warming up\"):\n",
        "      model_pt(**model_inputs_on_device)\n",
        "\n",
        "    # Compute\n",
        "    time_buffer = []\n",
        "    for _ in trange(100, desc=f\"Tracking inference time on PyTorch\"):\n",
        "      with track_infer_time(time_buffer):\n",
        "        model_pt(**model_inputs_on_device)\n",
        "\n",
        "    # Store the result\n",
        "    results[label] = OnnxInferenceResult(\n",
        "        time_buffer,\n",
        "        None\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoOL6J4sivyR"
      },
      "source": [
        "Benchmark the ONNX converted model on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPQ92mfxA5Qc"
      },
      "outputs": [],
      "source": [
        "PROVIDERS = {\n",
        "    (\"CUDAExecutionProvider\", \"ONNX GPU\"),\n",
        "}\n",
        "\n",
        "for provider, label in PROVIDERS:\n",
        "    model = create_model_for_provider(onnx_model_path, provider)\n",
        "\n",
        "    time_buffer = []\n",
        "\n",
        "    model.run(None, inputs_onnx)\n",
        "\n",
        "    for _ in trange(100, desc=f\"Tracking inference time on {provider}\"):\n",
        "      with track_infer_time(time_buffer):\n",
        "          model.run(None, inputs_onnx)\n",
        "\n",
        "    results[label] = OnnxInferenceResult(\n",
        "      time_buffer,\n",
        "      model.get_session_options().optimized_model_filepath\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EVp3pAei2c0"
      },
      "source": [
        "Benchmark the ONNX converted model on GPU doing IO binding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ3kY-3GElx3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "PROVIDERS = {\n",
        "    (\"CUDAExecutionProvider\", \"ONNX GPU IO Binding\"),\n",
        "}\n",
        "\n",
        "for provider, label in PROVIDERS:\n",
        "    model = create_model_for_provider(onnx_model_path, provider)\n",
        "    io_binding = model.io_binding()\n",
        "    io_binding.bind_input(\n",
        "      name='input_ids',\n",
        "      device_type='cuda',\n",
        "      device_id=0,\n",
        "      element_type=np.int64,\n",
        "      shape=tuple(model_inputs['input_ids'].shape),\n",
        "      buffer_ptr=model_inputs['input_ids'].data_ptr(),\n",
        "    )\n",
        "    io_binding.bind_input(\n",
        "      name='attention_mask',\n",
        "      device_type='cuda',\n",
        "      device_id=0,\n",
        "      element_type=np.int64,\n",
        "      shape=tuple(model_inputs['attention_mask'].shape),\n",
        "      buffer_ptr=model_inputs['attention_mask'].data_ptr(),\n",
        "    )\n",
        "\n",
        "    output_names = model.get_outputs()[0].name\n",
        "    io_binding.bind_output(output_names, 'cuda')\n",
        "\n",
        "    time_buffer = []\n",
        "\n",
        "    model.run_with_iobinding(io_binding)\n",
        "\n",
        "    for _ in trange(100, desc=f\"Tracking inference time on {provider}\"):\n",
        "      with track_infer_time(time_buffer):\n",
        "          model.run_with_iobinding(io_binding)\n",
        "\n",
        "    results[label] = OnnxInferenceResult(\n",
        "      time_buffer,\n",
        "      model.get_session_options().optimized_model_filepath\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9mpbuS96iqK"
      },
      "source": [
        "### Compare Benchmark Results Visually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ntF02WB6m2B"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Compute average inference time and standard deviation\n",
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "time_results_std = {k: np.std(v.model_inference_time) * 1000 for k, v in results.items()}\n",
        "\n",
        "fig = px.bar(x=time_results.keys(), y=time_results.values(),\n",
        "             title=\"Average inference time (ms) for each provider\",\n",
        "             labels={'x':'Provider', 'y':'Avg Inference time (ms)'},\n",
        "             color=time_results.values(),\n",
        "             color_continuous_scale=px.colors.sequential.Tealgrn,\n",
        "             text_auto='.2s')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ub9oRAcSgSl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(columns=['Provider', 'Inference_time'])\n",
        "for k, v in results.items():\n",
        "  for i in range(len(v.model_inference_time)):\n",
        "    results_df.loc[len(results_df.index)] = [k, v.model_inference_time[i] * 1e3]\n",
        "\n",
        "fig = px.box(results_df, x=\"Provider\", y=\"Inference_time\",\n",
        "             points=\"all\",\n",
        "             labels={'Provider':'Provider', 'Inference_time':'Inference durations (ms)'})\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
