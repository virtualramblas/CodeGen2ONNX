{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "id": "UkjzhF2z3s2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing requirements in the Colab VM."
      ],
      "metadata": {
        "id": "r7NTY8oA3zFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feOSSQy500_t"
      },
      "outputs": [],
      "source": [
        "!pip install transformers onnx onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the CodeGen pre-trained model and tokenizer. "
      ],
      "metadata": {
        "id": "8NdKa_u_8D-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "  device = \"cuda\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer_id = \"Salesforce/codegen-350M-mono\"\n",
        "model_id = \"Salesforce/codegen-350M-mono\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
      ],
      "metadata": {
        "id": "eHRjINkqpi5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"local-pt-checkpoint\")\n",
        "model.save_pretrained(\"local-pt-checkpoint\")"
      ],
      "metadata": {
        "id": "zYrw3gemqzr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversion to ONNX Format"
      ],
      "metadata": {
        "id": "ylUPrb198X4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the pre-trained model to the ONNX format using the tool available in the Transformers library. The command below performs also validation at the end of the conversion process."
      ],
      "metadata": {
        "id": "p7xl97n48co9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m transformers.onnx --feature \"causal-lm\" --framework pt --export_with_transformers --model=local-pt-checkpoint onnx/"
      ],
      "metadata": {
        "id": "6REXR-NZq22e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization"
      ],
      "metadata": {
        "id": "k4MKPa9U1mEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do 8-bit quantization of the ONNX converted model."
      ],
      "metadata": {
        "id": "T4aqW3tD1xW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"onnx/model.onnx\"\n",
        "quantized_model_path = \"model.quant.onnx\""
      ],
      "metadata": {
        "id": "KtqLH-8DtzW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "def quantize_onnx_model(onnx_model_path, quantized_model_path):    \n",
        "    onnx_opt_model = onnx.load(onnx_model_path)\n",
        "    quantize_dynamic(onnx_model_path,\n",
        "                     quantized_model_path,\n",
        "                     weight_type=QuantType.QInt8)\n",
        "\n",
        "quantize_onnx_model(onnx_model_path, quantized_model_path)\n",
        "\n",
        "print('ONNX full precision model size (MB):', os.path.getsize(onnx_model_path)/(1024*1024))\n",
        "print('ONNX quantized model size (MB):', os.path.getsize(quantized_model_path)/(1024*1024))"
      ],
      "metadata": {
        "id": "v2q0joI8_0zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmarks"
      ],
      "metadata": {
        "id": "K3Qz1eI-5zpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some utility functions to perform benchmarks of different versions of the model with diverse providers in the ONNX runtime."
      ],
      "metadata": {
        "id": "-Iy7lsqH96iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from time import time\n",
        "from tqdm import trange\n",
        "from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_all_providers\n",
        "\n",
        "def create_model_for_provider(model_path: str, provider: str, thread_pooling=False) -> InferenceSession: \n",
        "  \n",
        "  assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
        "\n",
        "  options = SessionOptions()\n",
        "  if thread_pooling:\n",
        "    options.intra_op_num_threads = 1\n",
        "  options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        " \n",
        "  session = InferenceSession(model_path, options, providers=[provider])\n",
        "  session.disable_fallback()\n",
        "    \n",
        "  return session\n",
        "\n",
        "@contextmanager\n",
        "def track_infer_time(buffer: [int]):\n",
        "    start = time()\n",
        "    yield\n",
        "    end = time()\n",
        "\n",
        "    buffer.append(end - start)\n",
        "\n",
        "@dataclass\n",
        "class OnnxInferenceResult:\n",
        "  model_inference_time: [int]  \n",
        "  optimized_model_path: str"
      ],
      "metadata": {
        "id": "d0TIpRSc2FNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the input to use for benchmarking the original model (PyTorch Tensor) and the ONNX versions (numpy array)."
      ],
      "metadata": {
        "id": "6KXkkPHoA-MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CodeGenTokenizerFast\n",
        "\n",
        "tokenizer = CodeGenTokenizerFast.from_pretrained(model_id)\n",
        "\n",
        "model_inputs = tokenizer(\"def hello_world():\", return_tensors=\"pt\")\n",
        "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}"
      ],
      "metadata": {
        "id": "BBFWvEaN2RzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark inference of the original PyTorch model on CPU."
      ],
      "metadata": {
        "id": "Rll78-9_5-RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CodeGenModel\n",
        "\n",
        "PROVIDERS = {\n",
        "    (\"cpu\", \"PyTorch CPU\"),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for device, label in PROVIDERS:\n",
        "    \n",
        "    model_inputs_on_device = {\n",
        "        arg_name: tensor.to(device)\n",
        "        for arg_name, tensor in model_inputs.items()\n",
        "    }\n",
        "\n",
        "    model_pt = CodeGenModel.from_pretrained(model_id).to(device)\n",
        "    for _ in trange(10, desc=\"Warming up\"):\n",
        "      model_pt(**model_inputs_on_device)\n",
        "\n",
        "    # Compute \n",
        "    time_buffer = []\n",
        "    for _ in trange(100, desc=f\"Tracking inference time on PyTorch\"):\n",
        "      with track_infer_time(time_buffer):\n",
        "        model_pt(**model_inputs_on_device)\n",
        "\n",
        "    # Store the result\n",
        "    results[label] = OnnxInferenceResult(\n",
        "        time_buffer, \n",
        "        None\n",
        "    ) "
      ],
      "metadata": {
        "id": "H8O2tJvt51am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark the converted model to ONNX format in the ONNX runtime (CPU)."
      ],
      "metadata": {
        "id": "WvZdOTvq6RHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROVIDERS = {\n",
        "    (\"CPUExecutionProvider\", \"ONNX CPU\"),\n",
        "}\n",
        "\n",
        "for provider, label in PROVIDERS:\n",
        "    model = create_model_for_provider(onnx_model_path, provider)\n",
        "\n",
        "    time_buffer = []\n",
        "\n",
        "    model.run(None, inputs_onnx)\n",
        " \n",
        "    for _ in trange(100, desc=f\"Tracking inference time on {provider}\"):\n",
        "      with track_infer_time(time_buffer):\n",
        "          model.run(None, inputs_onnx)\n",
        "\n",
        "    results[label] = OnnxInferenceResult(\n",
        "      time_buffer,\n",
        "      model.get_session_options().optimized_model_filepath\n",
        "    )"
      ],
      "metadata": {
        "id": "YBavLxtD6Rhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare benchmark results visually."
      ],
      "metadata": {
        "id": "D9mpbuS96iqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "# Compute average inference time\n",
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "\n",
        "fig = px.bar(x=time_results.keys(), y=time_results.values(), \n",
        "             title=\"Average inference time (ms) for each provider\", \n",
        "             labels={'x':'Provider', 'y':'Avg Inference time (ms)'},\n",
        "             text_auto='.2s')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PJVJ4PlABqkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantize and benchmark the original PyTorch model on CPU."
      ],
      "metadata": {
        "id": "Jycdtbn88oQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "model_pt_quantized = torch.quantization.quantize_dynamic(\n",
        "    model_pt.to(\"cpu\"), {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "model_pt_quantized(**model_inputs)\n",
        "\n",
        "time_buffer = []\n",
        "for _ in trange(100):\n",
        "    with track_infer_time(time_buffer):\n",
        "        model_pt_quantized(**model_inputs)\n",
        "    \n",
        "results[\"PyTorch CPU Quantized\"] = OnnxInferenceResult(\n",
        "    time_buffer,\n",
        "    None\n",
        ")"
      ],
      "metadata": {
        "id": "qBvH1-EG7Dz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark the ONNX quantized model in the ONNX runtime (CPU)."
      ],
      "metadata": {
        "id": "WVBfTvko82Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = create_model_for_provider(quantized_model_path, \"CPUExecutionProvider\")\n",
        "\n",
        "outputs = quantized_model.run(None, inputs_onnx)\n",
        "\n",
        "time_buffer = []\n",
        "for _ in trange(100, desc=f\"Tracking inference time on CPUExecutionProvider with quantized model\"):\n",
        "    with track_infer_time(time_buffer):\n",
        "        outputs = quantized_model.run(None, inputs_onnx)\n",
        "\n",
        "results[\"ONNX CPU Quantized\"] = OnnxInferenceResult(\n",
        "    time_buffer, \n",
        "    quantized_model_path\n",
        ") "
      ],
      "metadata": {
        "id": "cu3VENui83aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare all the benchmark results visually."
      ],
      "metadata": {
        "id": "I3ImcmPQ9fjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute average inference time and standard deviation\n",
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "time_results_std = {k: np.std(v.model_inference_time) * 1000 for k, v in results.items()}\n",
        "\n",
        "fig = px.bar(x=time_results.keys(), y=time_results.values(), \n",
        "             title=\"Average inference time (ms) for each provider\", \n",
        "             labels={'x':'Provider', 'y':'Avg Inference time (ms)'},\n",
        "             color=time_results.values(),\n",
        "             color_continuous_scale=px.colors.sequential.Tealgrn,\n",
        "             text_auto='.2s')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZQGKpNUpODG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(\n",
        "    name='Control',\n",
        "    x=list(time_results.keys()), y=list(time_results.values()),  \n",
        "    error_y=dict(type='data', array=list(time_results_std.values())),\n",
        "    marker=dict(\n",
        "        colorscale='Tealgrn',\n",
        "        showscale=True\n",
        "    )\n",
        "))\n",
        "fig.update_xaxes(title_text=\"Provider\")\n",
        "fig.update_yaxes(title_text=\"Avg Inference time (ms)\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "S9j268UdaTWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KK1aU9YcbxAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}